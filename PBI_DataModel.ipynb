{"cells":[{"cell_type":"markdown","source":["# Export PBI Metatdata to Lakehouse\n","\n","Version==1.0.1"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"41836ce4-8936-4a9b-ac8d-866491fa5066"},{"cell_type":"code","source":["%pip install semantic-link-labs==0.10.1\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3866882-1441-45d4-b8ab-2e7bde966c9d"},{"cell_type":"markdown","source":["### Configuration:\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c4d3d90-a24e-42f2-b9fc-b1a0855addd7"},{"cell_type":"code","source":["#filter by workspace list\n","workspace_list=[]\n","\n","#include personal workspace\n","include_personal_workspace=False\n","\n","#filter by PP3 workspace\n","only_PP3=True"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"76b3b0d0-d817-46f7-9dc5-34e2c40abeec"},{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","\n","import sempy\n","import sempy.fabric as fabric\n","import pandas as pd\n","import json\n","import re\n","import sempy_labs as labs\n","from sempy_labs import admin as labs_admin\n","from sempy_labs import report as labs_report\n","\n","\n","\n","def sanitize_name(name: str) -> str:\n","    \"\"\"\n","    Replace any character in the input string that is not a letter, number, or underscore with '_'.\n","    \"\"\"\n","    return re.sub(r'[^a-zA-Z0-9_]', '_', name)\n","def clean_name(dataframe):\n","    dataframe.columns = [col.replace(\" \", \"_\") for col in dataframe.columns]\n","\n","lakehouse=\"LH_\"+sanitize_name(notebookutils.runtime.context['currentNotebookName'])\n","\n","\n","#check if lakehouse exist\n","df_items =fabric.list_items()\n","lakehouse_exists = df_items['Display Name'].eq(lakehouse).any()\n","if lakehouse_exists:\n","    print(\"Lakehouse already exists.\")\n","    lhid = ( df_items[df_items['Display Name'].eq(lakehouse)&df_items['Type'].eq('Lakehouse')].iloc[0, 0])\n","else:\n","    print(\"Lakehouse does not exist. Creating...\")\n","    lhid = fabric.create_lakehouse(lakehouse)\n","\n","\n","all_capacities=labs_admin.list_capacities()\n","clean_name(all_capacities)\n","\n","labs.save_as_delta_table(dataframe=all_capacities, delta_table_name=\"Capacities\", write_mode=\"overwrite\",merge_schema=True,lakehouse=lakehouse)\n","\n","print(all_capacities.columns)\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5b24eab5-098d-44f9-a63e-acd325f0a236"},{"cell_type":"code","source":[" \n","    \n","all_workspaces_df = labs_admin.list_workspaces()\n","clean_name(all_workspaces_df)\n","all_workspaces_df['Comment']=''\n","all_capacities=all_capacities[['Capacity_Id','Capacity_Name','Sku','Region']]\n","print(all_capacities.columns)\n","print(all_workspaces_df.columns)\n","\n","all_workspaces_df=pd.merge(all_workspaces_df,all_capacities,left_on='Capacity_Id',right_on='Capacity_Id',how='left')\n","\n","def filter_workspace(ws_df, type):\n","    return ws_df.drop(ws_df[(ws_df.Type == type)].index)\n","\n","\n","all_workspaces_df=filter_workspace(all_workspaces_df,'AdminWorkspace')\n","if workspace_list:\n","    all_workspaces_df=all_workspaces_df[all_workspaces_df[\"Name\"].isin(workspace_list)]\n","\n","\n","#filter out PP3\n","if only_PP3:\n","    all_workspaces_df = all_workspaces_df[all_workspaces_df['Sku'] == 'PP3'].copy()\n","\n","if not include_personal_workspace:\n","    all_workspaces_df=filter_workspace(all_workspaces_df,'Personal')\n","    \n","\n","#all_workspaces_df=all_workspaces_df.drop(columns=[\"Capacity Id\"])\n","labs.save_as_delta_table(dataframe=all_workspaces_df, delta_table_name=\"Workspaces\", write_mode=\"overwrite\",merge_schema=True,lakehouse=lakehouse)\n","\n","\n","    \n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"91168538-defd-4166-b66a-6bdd46d08f09"},{"cell_type":"code","source":["# items_list=[]\n","# for _, ws_row in all_workspaces_df.iterrows():\n","#     ws_name=ws_row['Name']\n","#     #get items\n","#     df_items=labs_admin.list_items(workspace=ws_name)\n","#     df_items[\"Workspace_Name\"]=ws_name\n","#     items_list.append(df_items)\n","\n","# df_all_items=pd.concat(items_list,ignore_index=True)\n","# labs.save_as_delta_table(dataframe=df_all_items,delta_table_name=\"WorkspaceItems\",write_mode=\"overwrite\",merge_schema=True,lakehouse=lakehouse)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11a830ea-aabb-4ebb-8e28-a14b76d2eaec"},{"cell_type":"code","source":["df_items = labs_admin.list_items()\n","# clean_name(df_items)\n","# items_list=[]\n","# for _, ws_row in all_workspaces_df.iterrows():\n","#     ws_name=ws_row['Name']\n","#     #get items\n","#     df_items=labs_admin.list_items(workspace=ws_name)\n","#     df_items[\"Workspace_Name\"]=ws_name\n","#     items_list.append(df_items)\n","\n","# df_all_items=pd.concat(items_list,ignore_index=True)\n","labs.save_as_delta_table(dataframe=df_items,delta_table_name=\"WorkspaceItemsAll\",write_mode=\"overwrite\",merge_schema=True,lakehouse=lakehouse)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b2ae04b5-e0bf-4f73-878b-d9c31d0be957"},{"cell_type":"code","source":["df_semantic_models = labs_admin.list_datasets()\n","df_semantic_models[\"Upstream Datasets\"] = df_semantic_models[\"Upstream Datasets\"].astype(str)\n","df_semantic_models[\"Users\"] = df_semantic_models[\"Users\"].astype(str)\n","# df_large_models = df_semantic_models[df_semantic_models['Target Storage Mode'] == 'PremiumFiles'].copy()\n","# #df_large_models = df_large_models[df_large_models[\"Workspace Id\"].isin(all_workspaces_df['Id'])]\n","# import uuid\n","# import math\n","# for _, large_model_row in df_large_models.iterrows():\n","#     large_model_id = large_model_row[\"Dataset Id\"]\n","#     large_model_workspace_id = uuid.UUID(large_model_row[\"Workspace Id\"])\n","\n","#     if not labs.is_default_semantic_model(dataset=large_model_id, workspace=large_model_workspace_id):\n","#         model_size = labs.get_semantic_model_size(dataset=large_model_id, workspace=large_model_workspace_id)\n","#         df_large_models[\"Model Size\"] = round(model_size/(1024*1024*1024),3)\n","    \n","#     # You can add any processing logic here for each large model\n","#     print(f\"Processing large model: {large_model_id} in workspace: {large_model_workspace_id}\")\n","\n","\n","\n","labs.save_as_delta_table(dataframe=df_semantic_models,delta_table_name=\"SemanticModels\",write_mode=\"overwrite\",merge_schema=True,lakehouse=lakehouse)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"283af177-ceed-4148-910a-b58728a5125c"},{"cell_type":"code","source":["def get_model_prop(workspace):\n","    print(f\"process workspace {workspace}\")\n","    server = fabric.create_tom_server(readonly=True, workspace=workspace)\n","    print(f\"connect to {workspace}\")\n","    results=[]\n","    for d in server.Databases:\n","        r={}\n","        print(f\"{d.Name} {d.ID}  {round(d.EstimatedSize/(1024*1024*1024),3)}\")\n","        flist=\"\"\n","        for a in dir(d):\n","            try:\n","                if a.startswith(\"get_\") and a != \"get_SurrogatePairBehavior\" :\n","                    value=eval(f\"d.{a}()\")\n","                    name=a.replace(\"get_\",\"\")\n","                    if type(value) not in [str,int]:\n","                        value=str(value)\n","                    # print(f\"{name} {value} {type(value)}\")\n","                    flist+=name+\",\"\n","                    r[name]=value\n","            except Exception as e:\n","                print(e)\n","        #print(flist)\n","        r['Workspace']=workspace\n","        results.append(r)\n","    return pd.DataFrame(results)\n","all_models=[]\n"," \n","for index, ws_row in all_workspaces_df.iterrows():\n","    ws_name=ws_row['Name']\n","    ws_id=ws_row['Id']\n","    try:\n","        models_meta=get_model_prop(ws_name)\n","        all_models.append(models_meta)\n","    except Exception as e:\n","        all_workspaces_df.at[index,'Comment']=str(e)\n","        #print(e)\n","        print(f\"failed to process workspace {ws_name} {ws_id}\")\n","# print(all_models)\n","df_all_items=pd.concat(all_models,ignore_index=True)\n","# print(df_all_items)\n","labs.save_as_delta_table(dataframe=all_workspaces_df, delta_table_name=\"Workspaces\", write_mode=\"overwrite\",merge_schema=True,lakehouse=lakehouse)\n","if df_all_items is not None:\n","    labs.save_as_delta_table(dataframe=df_all_items,delta_table_name=\"SemanticModelsXmla\",write_mode=\"overwrite\",merge_schema=True,lakehouse=lakehouse)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"source_hidden":false}},"id":"f1345b3c-9a16-413d-a06c-5dadbfa64e3c"},{"cell_type":"code","source":["sql=f'''\n","\n","select m.Workspace_Id,x.Workspace ,m.Dataset_Id, m.Dataset_Name, m.Target_Storage_Mode,x.EstimatedSize/(1024*1024*1024) as Size from \n","{lakehouse}.SemanticModelsXmla x , {lakehouse}.SemanticModels m where x.ID=m.Dataset_Id\n"," and  m.Target_Storage_Mode = 'PremiumFiles'\n","'''\n","print(sql)\n","df = spark.sql(sql)\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b878913b-4d32-4892-be62-20d51ca47cf6"},{"cell_type":"markdown","source":["workspace license info pro:\n","\n","process workspace test model migration new\n","The '<euii>freeuser1@lgjtest.onmicrosoft.com</euii>' user does not have permission to call the Discover method.\n","\n","\n","capacity is not available\n","Server '977D1839-3AB0-4005-B31B-2B6CABBE0745' is not found.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"116e3f31-125e-42e7-8d1c-b3dbe92762e1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5e11c2f8-21f1-4c5a-ad5b-b0e3cfb8a750"}],"default_lakehouse":"5e11c2f8-21f1-4c5a-ad5b-b0e3cfb8a750","default_lakehouse_name":"LH_PBI_DataModel__3_","default_lakehouse_workspace_id":"9fad14f5-3993-4bac-a4e7-59bbce4adfc6"}}},"nbformat":4,"nbformat_minor":5}